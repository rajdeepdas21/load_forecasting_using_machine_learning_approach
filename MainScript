import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

#Load dataset, skip first metadata row
file_path = "/home/rajdeep/Documents/load_forecasting/Active Power Load _33_11KV_Godishala Substation.xlsx"
df = pd.read_excel(file_path, header=1)

#Relevant columns by index to avoid KeyError
df = df.iloc[:, 1:10]  # columns TIME to Humidity

# Rename columns
df.columns = ["TIME", "VOLTAGE", "CURRENT", "PF", "POWER_KW",
              "WEEKEND_WEEKDAY", "SEASON", "TEMP_F", "HUMIDITY"]

#Convert TIME to hour
df["HOUR"] = df["TIME"].astype(str).str.extract(r"(\d+)-").astype(int)

#Lag features for STLF (previous 3 hours)
for lag in range(1, 4):
    df[f"Load_t-{lag}"] = df["POWER_KW"].shift(lag)

df = df.dropna()

#Input features and target
feature_cols = ["HOUR", "VOLTAGE", "CURRENT", "PF", "WEEKEND_WEEKDAY",
                "SEASON", "TEMP_F", "HUMIDITY",
                "Load_t-1", "Load_t-2", "Load_t-3"]
X = df[feature_cols]
y = df["POWER_KW"]

#Scale features for models sensitive to scale
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

#Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

#ML models
models = {
    "Decision Tree": DecisionTreeRegressor(max_depth=5),
    "Random Forest": RandomForestRegressor(n_estimators=100),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100),
    "Linear SVM": SVR(kernel="linear"),
    "RBF SVM": SVR(kernel="rbf", gamma=0.1),
    "KNN (k=5)": KNeighborsRegressor(n_neighbors=5),
    "AdaBoost": AdaBoostRegressor(n_estimators=50),
    "Bagging (Tree)": BaggingRegressor(DecisionTreeRegressor(), n_estimators=50),
    "MLP (Neural Net)": MLPRegressor(hidden_layer_sizes=(20,), solver='lbfgs', max_iter=5000, tol=1e-6)
}

#Train and evaluate models
results = {}

for name, model in models.items():
    print(f"\nTraining {name} ...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    r = np.sqrt(r2) if r2 >= 0 else -np.sqrt(abs(r2))
    
    results[name] = [mae, mse, rmse, r2, r]
    print(f"MAE: {mae:.3f} | RMSE: {rmse:.3f} | R²: {r2:.3f} | R: {r:.3f}")

#Results to DataFrame
results_df = pd.DataFrame(results, index=["MAE", "MSE", "RMSE", "R²", "R"]).T

#Plot comparison
plt.figure(figsize=(16,8))
results_df[["MAE","RMSE","R²"]].plot(kind="bar")
plt.title("STLF Model Performance Comparison")
plt.ylabel("Error / Score")
plt.xticks(rotation=45)
plt.grid(axis="y")
plt.show()

#Display summary table
print("\nSTLF Model Performance Summary (MAE | MSE | RMSE | R² | R):")
print(results_df)
